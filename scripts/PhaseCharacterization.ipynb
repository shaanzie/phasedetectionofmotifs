{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Clusters')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/HiBench/csvs/3.2.1/terasort_run.csv\")\n",
    "data = data.drop(['Unnamed: 0'], axis = 1)\n",
    "data = data.dropna()\n",
    "# data \n",
    "IPC = list()\n",
    "for i in data.iterrows():\n",
    "    if(int(i[1][5]) == 0):\n",
    "        IPC.append(None)\n",
    "    else:\n",
    "        if(int(i[1][1]) / int(i[1][5]) < -0.4 or int(i[1][1]) / int(i[1][5]) > 1):\n",
    "            IPC.append(None)\n",
    "        else:\n",
    "            IPC.append(int(i[1][1]) / int(i[1][5]))\n",
    "#     print(i[1][4])\n",
    "\n",
    "data['IPC'] = IPC\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = data.values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "data = pd.DataFrame(x_scaled)\n",
    "\n",
    "# sse = []\n",
    "# list_k = list(range(1, 20))\n",
    "\n",
    "# for k in list_k:\n",
    "#     km = KMeans(n_clusters=k)\n",
    "#     km.fit(data)\n",
    "#     sse.append(km.inertia_)\n",
    "\n",
    "# # Plot sse against k\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(list_k, sse, '-o')\n",
    "# plt.xlabel(r'Number of clusters *k*')\n",
    "# plt.ylabel('Sum of squared distance');\n",
    "\n",
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=9)\n",
    "# Fitting the input data\n",
    "kmeans = kmeans.fit(data)\n",
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(data)\n",
    "# Centroid values\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "arr = []\n",
    "for i in range(len(labels)):\n",
    "    arr.append(i)\n",
    "# print(arr)\n",
    "plt.scatter(arr, labels, c='#050505', s=7)\n",
    "plt.xlabel(\"Time(s)\\ntiny_sort_prep\")\n",
    "plt.ylabel(\"Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Current Cluster</th>\n",
       "      <th>Next Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122578</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184414</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>0.295166</td>\n",
       "      <td>0.410972</td>\n",
       "      <td>0.415218</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417736</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149674</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.194539</td>\n",
       "      <td>0.401805</td>\n",
       "      <td>0.534092</td>\n",
       "      <td>0.143095</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389543</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290563</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.529605</td>\n",
       "      <td>0.725636</td>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.256825</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385361</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.214896</td>\n",
       "      <td>0.211615</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.136036</td>\n",
       "      <td>0.272686</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.656963</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.056615</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.040176</td>\n",
       "      <td>0.125481</td>\n",
       "      <td>0.032763</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.660491</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.188830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.049597</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.121193</td>\n",
       "      <td>0.099192</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.042415</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.406297</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.099770</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.181439</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.000492  0.000480  0.002895  0.006325  0.007090  0.001951  0.000051   \n",
       "1    0.001699  0.001526  0.003426  0.004929  0.007387  0.002809  0.000043   \n",
       "2    0.184414  0.199815  0.295166  0.410972  0.415218  0.164890  0.000997   \n",
       "3    0.149674  0.164176  0.194539  0.401805  0.534092  0.143095  0.001172   \n",
       "4    0.290563  0.291288  0.529605  0.725636  0.692073  0.256825  0.001425   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "513  0.214896  0.211615  0.190894  0.136036  0.272686  0.119368  0.000312   \n",
       "514  0.056615  0.059945  0.060155  0.040176  0.125481  0.032763  0.000098   \n",
       "515  0.002819  0.002917  0.006295  0.007061  0.005291  0.003840  0.000523   \n",
       "516  0.049597  0.051149  0.121193  0.099192  0.051149  0.042415  0.001036   \n",
       "517  0.116715  0.099770  0.048171  0.081230  0.181439  0.076565  0.001124   \n",
       "\n",
       "            7         8         9        10        11  Current Cluster  \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.052351                6   \n",
       "1    0.000013  0.000000  0.000000  0.000000  0.122578                6   \n",
       "2    0.000877  0.012987  0.000000  0.000000  0.417736                8   \n",
       "3    0.000990  0.066378  0.000000  0.000000  0.389543                2   \n",
       "4    0.001473  0.066378  0.000000  0.000000  0.385361                8   \n",
       "..        ...       ...       ...       ...       ...              ...   \n",
       "513  0.000243  0.722944  0.957532  0.096714  0.656963                5   \n",
       "514  0.001036  0.727273  0.957532  0.096714  0.660491                5   \n",
       "515  0.001621  0.727273  0.957532  0.096714  0.188830                5   \n",
       "516  0.001726  0.725830  0.957532  0.096714  0.406297                5   \n",
       "517  0.000139  0.725830  0.957532  0.096714  0.452888                5   \n",
       "\n",
       "     Next Cluster  \n",
       "0               6  \n",
       "1               8  \n",
       "2               2  \n",
       "3               8  \n",
       "4               8  \n",
       "..            ...  \n",
       "513             5  \n",
       "514             5  \n",
       "515             5  \n",
       "516             5  \n",
       "517             5  \n",
       "\n",
       "[518 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Current Cluster'] = labels\n",
    "to_cluster = labels[1:]\n",
    "to_cluster = np.append(to_cluster, [labels[-1]])\n",
    "len(to_cluster)\n",
    "data['Next Cluster'] = to_cluster\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Next Cluster'], axis = 1)\n",
    "y = data['Next Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 104 414 104\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>Next Cluster</td>   <th>  R-squared (uncentered):</th>      <td>   0.880</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.877</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   284.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 04 Apr 2020</td> <th>  Prob (F-statistic):</th>          <td>1.26e-222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:35:50</td>     <th>  Log-Likelihood:    </th>          <td> -902.70</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   518</td>      <th>  AIC:               </th>          <td>   1831.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   505</td>      <th>  BIC:               </th>          <td>   1887.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>               <td>   -0.4587</td> <td>    2.739</td> <td>   -0.167</td> <td> 0.867</td> <td>   -5.841</td> <td>    4.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>               <td>  -10.3585</td> <td>    4.147</td> <td>   -2.498</td> <td> 0.013</td> <td>  -18.507</td> <td>   -2.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>               <td>   -0.3104</td> <td>    1.043</td> <td>   -0.298</td> <td> 0.766</td> <td>   -2.360</td> <td>    1.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>               <td>    3.0225</td> <td>    0.940</td> <td>    3.215</td> <td> 0.001</td> <td>    1.176</td> <td>    4.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>               <td>   -0.9954</td> <td>    0.592</td> <td>   -1.683</td> <td> 0.093</td> <td>   -2.158</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>               <td>   10.3625</td> <td>    3.186</td> <td>    3.252</td> <td> 0.001</td> <td>    4.102</td> <td>   16.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>               <td>    0.2086</td> <td>    1.153</td> <td>    0.181</td> <td> 0.857</td> <td>   -2.057</td> <td>    2.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>               <td>   -0.6776</td> <td>    1.025</td> <td>   -0.661</td> <td> 0.509</td> <td>   -2.692</td> <td>    1.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>               <td>    0.9189</td> <td>    0.256</td> <td>    3.590</td> <td> 0.000</td> <td>    0.416</td> <td>    1.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>               <td>    0.1602</td> <td>    0.279</td> <td>    0.575</td> <td> 0.566</td> <td>   -0.387</td> <td>    0.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>              <td>   -0.7102</td> <td>    0.228</td> <td>   -3.118</td> <td> 0.002</td> <td>   -1.158</td> <td>   -0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>              <td>    3.1733</td> <td>    0.427</td> <td>    7.438</td> <td> 0.000</td> <td>    2.335</td> <td>    4.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Current Cluster</th> <td>    0.6673</td> <td>    0.033</td> <td>   20.324</td> <td> 0.000</td> <td>    0.603</td> <td>    0.732</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>86.615</td> <th>  Durbin-Watson:     </th> <td>   2.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 630.148</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.484</td> <th>  Prob(JB):          </th> <td>1.46e-137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.316</td> <th>  Cond. No.          </th> <td>    376.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:           Next Cluster   R-squared (uncentered):                   0.880\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.877\n",
       "Method:                 Least Squares   F-statistic:                              284.5\n",
       "Date:                Sat, 04 Apr 2020   Prob (F-statistic):                   1.26e-222\n",
       "Time:                        12:35:50   Log-Likelihood:                         -902.70\n",
       "No. Observations:                 518   AIC:                                      1831.\n",
       "Df Residuals:                     505   BIC:                                      1887.\n",
       "Df Model:                          13                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "0                  -0.4587      2.739     -0.167      0.867      -5.841       4.923\n",
       "1                 -10.3585      4.147     -2.498      0.013     -18.507      -2.210\n",
       "2                  -0.3104      1.043     -0.298      0.766      -2.360       1.739\n",
       "3                   3.0225      0.940      3.215      0.001       1.176       4.869\n",
       "4                  -0.9954      0.592     -1.683      0.093      -2.158       0.167\n",
       "5                  10.3625      3.186      3.252      0.001       4.102      16.623\n",
       "6                   0.2086      1.153      0.181      0.857      -2.057       2.474\n",
       "7                  -0.6776      1.025     -0.661      0.509      -2.692       1.336\n",
       "8                   0.9189      0.256      3.590      0.000       0.416       1.422\n",
       "9                   0.1602      0.279      0.575      0.566      -0.387       0.708\n",
       "10                 -0.7102      0.228     -3.118      0.002      -1.158      -0.263\n",
       "11                  3.1733      0.427      7.438      0.000       2.335       4.012\n",
       "Current Cluster     0.6673      0.033     20.324      0.000       0.603       0.732\n",
       "==============================================================================\n",
       "Omnibus:                       86.615   Durbin-Watson:                   2.115\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              630.148\n",
       "Skew:                           0.484   Prob(JB):                    1.46e-137\n",
       "Kurtosis:                       8.316   Cond. No.                         376.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvergenceMonitor(\n",
       "    history=[5229.627212694276, 1548.256350898749],\n",
       "    iter=8,\n",
       "    n_iter=100,\n",
       "    tol=0.01,\n",
       "    verbose=False,\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "np.random.seed(42)\n",
    "X = data[['Current Cluster', 'Next Cluster']]\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", n_iter=100)\n",
    "model.fit(X)\n",
    "model.score(X)\n",
    "model.monitor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 5, 3, 7, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 9, 3, 7, 7, 7, 7, 3,\n",
       "       4, 5, 3, 4, 4, 4, 9, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 3, 5, 0, 0, 0,\n",
       "       8, 8, 0, 3, 5, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 8, 0, 0, 3,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       3, 7, 7, 7, 5, 0, 0, 0, 0, 2, 1, 3, 7, 7, 7, 5, 0, 0, 0, 0, 0, 0,\n",
       "       2, 0, 0, 3, 7, 5, 2, 0, 0, 0, 3, 5, 1, 2, 1, 2, 3, 7, 9, 3, 7, 7,\n",
       "       7, 7, 9, 8, 0, 3, 7, 9, 8, 0, 0, 8, 3, 7, 7, 7, 9, 8, 0, 8, 0, 0,\n",
       "       0, 8, 0, 8, 0, 0, 3, 7, 7, 7, 9, 8, 0, 8, 0, 0, 3, 7, 7, 7, 9, 0,\n",
       "       8, 0, 0, 3, 7, 7, 7, 9, 0, 8, 0, 0, 3, 9, 0, 8, 0, 0, 3, 7, 9, 8,\n",
       "       0, 8, 0, 0, 3, 7, 7, 9, 8, 0, 8, 0, 0, 3, 7, 7, 7, 9, 8, 0, 3, 7,\n",
       "       9, 8, 0, 0, 3, 7, 7, 7, 9, 0, 8, 0, 0, 3, 7, 7, 7, 9, 0, 0, 8, 0,\n",
       "       8, 3, 9, 0, 0, 8, 0, 3, 7, 7, 7, 7, 7, 9, 8, 0, 0, 0, 8, 8, 3, 9,\n",
       "       0, 0, 8, 0, 3, 7, 7, 7, 7, 9, 0, 0, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current Cluster</th>\n",
       "      <th>Next Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Current Cluster  Next Cluster\n",
       "0                  6             6\n",
       "1                  6             8\n",
       "2                  8             2\n",
       "3                  2             8\n",
       "4                  8             8\n",
       "..               ...           ...\n",
       "513                5             5\n",
       "514                5             5\n",
       "515                5             5\n",
       "516                5             5\n",
       "517                5             5\n",
       "\n",
       "[518 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 52 466 52\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "ncols = X_train.shape[1]\n",
    "\n",
    "model.add(Dense(10, activation = 'relu', input_shape = (ncols,)))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='Adamax',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shaanzie/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 466 samples, validate on 52 samples\n",
      "WARNING:tensorflow:From /home/shaanzie/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shaanzie/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/40\n",
      "466/466 [==============================] - 1s 2ms/step - loss: 15.8113 - accuracy: 0.0365 - val_loss: 12.3907 - val_accuracy: 0.0385\n",
      "WARNING:tensorflow:From /home/shaanzie/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 12.8197 - accuracy: 0.0365 - val_loss: 9.8236 - val_accuracy: 0.0385\n",
      "Epoch 3/40\n",
      "466/466 [==============================] - 0s 27us/step - loss: 10.1010 - accuracy: 0.0386 - val_loss: 7.2896 - val_accuracy: 0.0385\n",
      "Epoch 4/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 7.2910 - accuracy: 0.1867 - val_loss: 4.7360 - val_accuracy: 0.2308\n",
      "Epoch 5/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 4.6723 - accuracy: 0.2940 - val_loss: 2.7733 - val_accuracy: 0.2500\n",
      "Epoch 6/40\n",
      "466/466 [==============================] - 0s 26us/step - loss: 2.6563 - accuracy: 0.3369 - val_loss: 1.3476 - val_accuracy: 0.4423\n",
      "Epoch 7/40\n",
      "466/466 [==============================] - 0s 25us/step - loss: 1.2624 - accuracy: 0.4099 - val_loss: 0.5164 - val_accuracy: 0.4423\n",
      "Epoch 8/40\n",
      "466/466 [==============================] - 0s 24us/step - loss: 0.5182 - accuracy: 0.6073 - val_loss: 0.1882 - val_accuracy: 0.7308\n",
      "Epoch 9/40\n",
      "466/466 [==============================] - 0s 24us/step - loss: 0.2398 - accuracy: 0.7425 - val_loss: 0.1157 - val_accuracy: 0.8654\n",
      "Epoch 10/40\n",
      "466/466 [==============================] - 0s 26us/step - loss: 0.1616 - accuracy: 0.8369 - val_loss: 0.1110 - val_accuracy: 0.9231\n",
      "Epoch 11/40\n",
      "466/466 [==============================] - 0s 26us/step - loss: 0.1453 - accuracy: 0.8712 - val_loss: 0.1083 - val_accuracy: 0.9231\n",
      "Epoch 12/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.1340 - accuracy: 0.8948 - val_loss: 0.1011 - val_accuracy: 0.9231\n",
      "Epoch 13/40\n",
      "466/466 [==============================] - 0s 27us/step - loss: 0.1230 - accuracy: 0.9185 - val_loss: 0.0923 - val_accuracy: 0.9423\n",
      "Epoch 14/40\n",
      "466/466 [==============================] - 0s 22us/step - loss: 0.1123 - accuracy: 0.9378 - val_loss: 0.0829 - val_accuracy: 0.9423\n",
      "Epoch 15/40\n",
      "466/466 [==============================] - 0s 22us/step - loss: 0.1023 - accuracy: 0.9506 - val_loss: 0.0752 - val_accuracy: 0.9615\n",
      "Epoch 16/40\n",
      "466/466 [==============================] - 0s 38us/step - loss: 0.0924 - accuracy: 0.9549 - val_loss: 0.0681 - val_accuracy: 0.9808\n",
      "Epoch 17/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0834 - accuracy: 0.9592 - val_loss: 0.0607 - val_accuracy: 0.9808\n",
      "Epoch 18/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0743 - accuracy: 0.9635 - val_loss: 0.0535 - val_accuracy: 0.9808\n",
      "Epoch 19/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0657 - accuracy: 0.9657 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "466/466 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.96 - 0s 23us/step - loss: 0.0569 - accuracy: 0.9700 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "466/466 [==============================] - 0s 24us/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0434 - accuracy: 0.9893 - val_loss: 0.0300 - val_accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "466/466 [==============================] - 0s 25us/step - loss: 0.0379 - accuracy: 0.9957 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0329 - accuracy: 0.9957 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "466/466 [==============================] - 0s 22us/step - loss: 0.0277 - accuracy: 0.9957 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0224 - accuracy: 0.9957 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "466/466 [==============================] - 0s 24us/step - loss: 0.0179 - accuracy: 0.9957 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "466/466 [==============================] - 0s 22us/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "466/466 [==============================] - 0s 25us/step - loss: 0.0107 - accuracy: 0.9957 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "466/466 [==============================] - 0s 26us/step - loss: 0.0086 - accuracy: 0.9957 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "466/466 [==============================] - 0s 24us/step - loss: 0.0072 - accuracy: 0.9957 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "466/466 [==============================] - 0s 22us/step - loss: 0.0061 - accuracy: 0.9957 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0053 - accuracy: 0.9957 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0046 - accuracy: 0.9957 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0041 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0037 - accuracy: 0.9957 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "466/466 [==============================] - 0s 24us/step - loss: 0.0034 - accuracy: 0.9957 - val_loss: 8.4507e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0032 - accuracy: 0.9957 - val_loss: 7.0919e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0030 - accuracy: 0.9957 - val_loss: 5.9865e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "466/466 [==============================] - 0s 23us/step - loss: 0.0028 - accuracy: 0.9957 - val_loss: 5.3746e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd7a4a65240>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test), callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())\n",
    "data['Predictions'] = np.rint(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Current Cluster</th>\n",
       "      <th>Next Cluster</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122578</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184414</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>0.295166</td>\n",
       "      <td>0.410972</td>\n",
       "      <td>0.415218</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417736</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149674</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.194539</td>\n",
       "      <td>0.401805</td>\n",
       "      <td>0.534092</td>\n",
       "      <td>0.143095</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389543</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290563</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.529605</td>\n",
       "      <td>0.725636</td>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.256825</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385361</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.214896</td>\n",
       "      <td>0.211615</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.136036</td>\n",
       "      <td>0.272686</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.656963</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.056615</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.040176</td>\n",
       "      <td>0.125481</td>\n",
       "      <td>0.032763</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.660491</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.188830</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.049597</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.121193</td>\n",
       "      <td>0.099192</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.042415</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.406297</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.099770</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.181439</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.000492  0.000480  0.002895  0.006325  0.007090  0.001951  0.000051   \n",
       "1    0.001699  0.001526  0.003426  0.004929  0.007387  0.002809  0.000043   \n",
       "2    0.184414  0.199815  0.295166  0.410972  0.415218  0.164890  0.000997   \n",
       "3    0.149674  0.164176  0.194539  0.401805  0.534092  0.143095  0.001172   \n",
       "4    0.290563  0.291288  0.529605  0.725636  0.692073  0.256825  0.001425   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "513  0.214896  0.211615  0.190894  0.136036  0.272686  0.119368  0.000312   \n",
       "514  0.056615  0.059945  0.060155  0.040176  0.125481  0.032763  0.000098   \n",
       "515  0.002819  0.002917  0.006295  0.007061  0.005291  0.003840  0.000523   \n",
       "516  0.049597  0.051149  0.121193  0.099192  0.051149  0.042415  0.001036   \n",
       "517  0.116715  0.099770  0.048171  0.081230  0.181439  0.076565  0.001124   \n",
       "\n",
       "            7         8         9        10        11  Current Cluster  \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.052351                6   \n",
       "1    0.000013  0.000000  0.000000  0.000000  0.122578                6   \n",
       "2    0.000877  0.012987  0.000000  0.000000  0.417736                5   \n",
       "3    0.000990  0.066378  0.000000  0.000000  0.389543                5   \n",
       "4    0.001473  0.066378  0.000000  0.000000  0.385361                5   \n",
       "..        ...       ...       ...       ...       ...              ...   \n",
       "513  0.000243  0.722944  0.957532  0.096714  0.656963                7   \n",
       "514  0.001036  0.727273  0.957532  0.096714  0.660491                4   \n",
       "515  0.001621  0.727273  0.957532  0.096714  0.188830                4   \n",
       "516  0.001726  0.725830  0.957532  0.096714  0.406297                4   \n",
       "517  0.000139  0.725830  0.957532  0.096714  0.452888                4   \n",
       "\n",
       "     Next Cluster  Predictions  \n",
       "0               6          6.0  \n",
       "1               5          5.0  \n",
       "2               5          5.0  \n",
       "3               5          5.0  \n",
       "4               5          5.0  \n",
       "..            ...          ...  \n",
       "513             4          3.0  \n",
       "514             4          4.0  \n",
       "515             4          4.0  \n",
       "516             4          4.0  \n",
       "517             4          4.0  \n",
       "\n",
       "[518 rows x 15 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
